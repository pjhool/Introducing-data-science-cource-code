{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Perceptron example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training succesfull\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class perceptron():   #A\n",
    "    def __init__(self, X,y, threshold = 0.5, learning_rate = 0.1, max_epochs = 10): #B\n",
    "        self.threshold = threshold  #C\n",
    "        self.learning_rate = learning_rate  #D\n",
    "        self.X = X  #E\n",
    "        self.y = y  #E\n",
    "        self.max_epochs = max_epochs  #F\n",
    "\n",
    "    def initialize(self, init_type = 'zeros'):  #G\n",
    "        if init_type == 'random':  \n",
    "            self.weights = np.random.rand(len(self.X[0])) * 0.05 \n",
    "        if init_type == 'zeros':   \n",
    "            self.weights = np.zeros(len(self.X[0])) \n",
    "\n",
    "    def train(self):   #H\n",
    "        epoch = 0 #I\n",
    "        while True: #J\n",
    "            error_count = 0   #K\n",
    "            epoch += 1 #L\n",
    "            for (X,y) in zip(self.X, self.y): #M\n",
    "                error_count += self.train_observation(X,y,error_count) \n",
    "            if error_count == 0: #N\n",
    "                print \"training succesfull\"\n",
    "                break \n",
    "            if epoch >= self.max_epochs: #O\n",
    "                print \"reached maximum epochs, no perfect prediction\"\n",
    "                break\n",
    "\n",
    "    def train_observation(self,X,y, error_count):   #P\n",
    "        result = np.dot(X, self.weights) > self.threshold #Q\n",
    "        error = y - result #R\n",
    "\n",
    "        if error != 0:   #S\n",
    "            error_count += 1 #T\n",
    "            for index, value in enumerate(X): #U\n",
    "                self.weights[index] +=  self.learning_rate * error * value #V\n",
    "        return error_count #W\n",
    "\n",
    "    def predict(self, X):  #X\n",
    "        return int(np.dot(X, self.weights) > self.threshold) #Y\n",
    "\n",
    "X = [(1,0,0),(1,1,0),(1,1,1),(1,1,1),(1,0,1),(1,0,1)] #Z\n",
    "y = [1,1,0,0,1,1]#AA\n",
    "\n",
    "p = perceptron(X,y) #AB\n",
    "p.initialize() #AC\n",
    "p.train() #AD\n",
    "print p.predict((1,1,1)) #AE\n",
    "print p.predict((1,0,1)) #AE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A Set up the perceptron class\n",
    "#B the __init__ method of any python class is always ran when creating an instance of the class. Some default values are set here \n",
    "#C the threshold is an arbitrary cutoff between 0 and 1 to decide whether the prediction becomes a 0 or a 1. often its just 0.5, right in the middle but it depends on the use case.\n",
    "#D the learning rate of an algorithm is the adjustment it makes every time a new observation comes in. If this is high the model will adjust quickly to new observations but might \"overshoot\" and never get very precise. \n",
    "#oversimplified example: the optimal (and unknown) weight for an x-variable = 0.75. current estimation is 0.4. with a learning rate of 0.5 , the adjustment = 0.5 (learning rate) * 1 (size of error) *  1 (value of x) = 0.5. 0.4 (current weight) + 0.5 (adjustment) = 0.9 (new weight). Instead of 0.75. the adjustment was too big to get the correct result. \n",
    "#E x and y variables data are assigned to the class\n",
    "#F one epoch is one run through all the data. We allow for a maximum of 10 runs until we stop the perceptron. \n",
    "#G each observation will end up with a weight. the initialize function sets these weights for each incoming observation. We allow for two options: all weights start at 0 or they are assigned a small (between 0 and 0.05) random weight \n",
    "#H the training function\n",
    "#I we start at the first epoch. one epoch is one run through all the data.\n",
    "#J true is always true so technically this is a never ending loop but we build in several stop (break) conditions.\n",
    "#K initiate the number of encountered errors at 0 for each epoch. This is important, if an epoch ends without errors, the algorithm converged and we are done. \n",
    "#L add one to the current number of epochs\n",
    "#M we loop through the data and feed it to the train observation function, one observation at a time\n",
    "#N if by the end of the epoch we don't have an error, the training was successful\n",
    "#O if we reach the maximum number of allowed runs, we stop looking for a solution.\n",
    "#P the train observation function is run for every observation and will adjust the weights using the formula explained earlier.\n",
    "#Q A prediction is made for this observation. Because its binary this will be either 0 or 1.\n",
    "#R The real value (y) is either 0 or 1, the prediction is also 0 or 1. If it's wrong we get an error of either 1 or -1\n",
    "#S In case we have a wrong prediction (an error) we need to adjust the model\n",
    "#T ad 1 to the error count\n",
    "#U For every predictor variable in the input vector (X), we will adjust their weight. \n",
    "#V adjust the weight for every predictor variable using the learning rate , the error and the actual value of the predictor variable\n",
    "#W we return the error count because we need to evaluate it at the end of the epoch\n",
    "#X The predict class\n",
    "#Y The values of the predictor values is multiplied by their respective weights (this multiplication is done by np.dot). Then the outcome is \n",
    "# compared to the overall threshold (here this is 0.5) to see is a 0 or 1 should be predicted.\n",
    "#Z Our X (predictors) data matrix \n",
    "#AA Our y (target) data vector\n",
    "#AB we instantiate our perceptron class with the data from matrix X and vector y\n",
    "#AC The weights for the predictors is initialized (as explained higher up)\n",
    "#AD the perceptron model is trained. It will try to train until it either converges (no more errors) or it runs out of training runs (epochs)\n",
    "#AE we check what the perceptron would now predict given different values for the predictor variables. In the first case\n",
    "#it will predict 0, in the second it predicts a 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Block matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β=〖(X^T X)〗^(-1) X^T y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   1.00000001e+00   2.00000003e+00 ...,   4.99700007e+03\n",
      "   4.99800007e+03   4.99900007e+03]\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import bcolz as bc\n",
    "import numpy as np\n",
    "import dask\n",
    "\n",
    "n = 1e4 #A\n",
    "\n",
    "ar = bc.carray(np.arange(n).reshape(n/2,2)  , dtype='float64', rootdir = 'ar.bcolz', mode = 'w') #B\n",
    "y  = bc.carray(np.arange(n/2), dtype='float64', rootdir = 'yy.bcolz', mode = 'w') #B,\n",
    "\n",
    "dax = da.from_array(ar, chunks=(5,5)) #C\n",
    "dy = da.from_array(y,chunks=(5,5)) #C\n",
    "\n",
    "XTX = dax.T.dot(dax) #D\n",
    "Xy  = dax.T.dot(dy) #E\n",
    "\n",
    "coefficients = np.linalg.inv(XTX.compute()).dot(Xy.compute()) #F\n",
    "\n",
    "coef = da.from_array(coefficients,chunks=(5,5)) #G\n",
    "\n",
    "ar.flush() #H\n",
    "y.flush() #H\n",
    "\n",
    "predictions = dax.dot(coef).compute() #I\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A Number of observations (scientific notation). 1e4 = 10.000 , feel free to change this. \n",
    "# B create fake data: np.arange(n).reshape(n/2,2) creates a matrix of 5000 by 2 (because we set n to 10.000)\n",
    "# bc.carray = numpy array extension that can swap to disc. This is also stored in a compressed way)\n",
    "# rootdir = 'ar.bcolz'  --> create file on disc for in case he goes out of RAM. \n",
    "# you can check this on your file system next to this ipython file or wherever location you ran this code from\n",
    "# mode = 'w' --> write mode\n",
    "# dtype = 'float64' --> storage type of the data (which are float numbers)\n",
    "# C Here, block matrices are created for the predictor variables (ar) and target (y). \n",
    "# a block matrix is a matrix cut in pieces (the blocks) \n",
    "# da.from_array() reads data from disc or RAM (wherever it resides currently)\n",
    "# chunks=(5,5) every block is a 5 * 5 matrix , (unless < 5 observations or variables are left)\n",
    "#D The XTX is defined (defining it is \"lazy\") as the X matrix multiplied with its transposed version.\n",
    "#This is a building block of the formula to  do linear regression using matrix calculation\n",
    "#E Xy is the y vector multiplied with the transposed X matrix. Again the matrix is only defined, not calculated yet.\n",
    "#This is also a building block of the formula todo linear regression using matrix calculation (see formula)\n",
    "#F the coefficients are calculated using the matrix linear regression function. \n",
    "# np.linalg.inv() is the ^(-1) in this function or \"inversion\" of the matrix\n",
    "# X.dot(Y) --> multiply the matrix X with another matrix Y\n",
    "#G the coefficients are also put into a block matrix\n",
    "#we got a numpy array back from last step so we need to explicitly convert it back to a \"da array\".\n",
    "#H flush memory data, it's no longer needed to have large matrices in memory\n",
    "#I score the model (make predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generating out of memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 121 files\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-081e8248480a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"there are %d files\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_svmlight_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3231952\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Gebruiker\\Anaconda\\envs\\book\\lib\\site-packages\\scipy\\sparse\\base.pyc\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;33m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m         \"\"\"\n\u001b[1;32m--> 605\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gebruiker\\Anaconda\\envs\\book\\lib\\site-packages\\scipy\\sparse\\compressed.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;31m##############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gebruiker\\Anaconda\\envs\\book\\lib\\site-packages\\scipy\\sparse\\coo.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gebruiker\\Anaconda\\envs\\book\\lib\\site-packages\\scipy\\sparse\\base.pyc\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "files = glob.glob('C:\\Users\\Gebruiker\\Downloads\\url_svmlight.tar\\url_svmlight\\*.svm') #A\n",
    "files = glob.glob('C:\\Users\\Gebruiker\\Downloads\\url_svmlight\\url_svmlight\\*.svm') #B\n",
    "\n",
    "print \"there are %d files\" % len(files) #C\n",
    "X,y = load_svmlight_file(files[0] ,n_features=3231952) #D\n",
    "X.todense() #E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A point to files (linux)\n",
    "#B point to files (windows: tar file needs to be untarred first)\n",
    "#C indication of number of files\n",
    "#D load in the files\n",
    "#E The data is a big but sparse matrix. by turning it into a dense matrix (every 0 is represented in the file) we create an out of memory error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##check % of non-zero data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non-zero entries 0.000036\n"
     ]
    }
   ],
   "source": [
    "print \"number of non-zero entries %2.6f\"  % float((X.nnz)/(float(X.shape[0]) * float(X.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Recognising malicious urls Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 1: Research goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The goal of this case study is to recognize malicious urls amongst a large set of normal urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 2: data retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download data from http://sysnet.ucsd.edu/projects/url/#datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uri = 'D:\\Python Book\\Chapter 4\\url_svmlight.tar.gz' #A\n",
    "tar = tarfile.open(uri,\"r:gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A The uri variable holds the location you saved the downloaded files. You will need to fill this uri variable out yourself for the code to run on your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 3: Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no real data cleansing is necessary here, the urls were pre-cleansed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 4: data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We need to find out the dimensions of our data without uncompressing everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extracting url_svmlight,f size 0\n",
      " extracting url_svmlight/Day33.svm,f size 18674876\n",
      " extracting url_svmlight/Day32.svm,f size 18599211\n",
      " extracting url_svmlight/Day53.svm,f size 18963938\n",
      " extracting url_svmlight/Day20.svm,f size 18633460\n",
      " extracting url_svmlight/Day7.svm,f size 18777054\n",
      " extracting url_svmlight/Day117.svm,f size 18106370\n",
      "max X = 3231952, max y dimension = 20000\n"
     ]
    }
   ],
   "source": [
    "max_obs = 0 #B\n",
    "max_vars = 0 #C\n",
    "i = 0 #D\n",
    "split = 5 #E\n",
    "for tarinfo in tar: #F\n",
    "    print \" extracting %s,f size %s\" % (tarinfo.name, tarinfo.size)\n",
    "    if tarinfo.isfile():\n",
    "        f = tar.extractfile(tarinfo.name)#G\n",
    "        X,y = load_svmlight_file(f) #H\n",
    "        max_vars = np.maximum(max_vars, X.shape[0])#I\n",
    "        max_obs = np.maximum(max_obs, X.shape[1])#I\n",
    "        \n",
    "    if i  > split:\n",
    "        break  #J\n",
    "    i+= 1\n",
    "          \n",
    "print \"max X = %s, max y dimension = %s\" % (max_obs, max_vars ) #K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A The uri variable holds the location you saved the downloaded files. You will need to fill this uri variable out yourself for the code to run on your computer\n",
    "#B we don't know how many observations we have, let's initialize it at 0\n",
    "#C we don't know how many features we have, let's initialize it at 0\n",
    "#D stop at the twentieth file (instead of all of them: for demonstration purposes)\n",
    "#E initialize filecounter at 0\n",
    "#F All files together take up around 2.05 Gb. The trick here is to leave the data compressed in main memory, only unpack what you need\n",
    "#G We unpack the files one by one to reduce the memory needed\n",
    "#H Use a helper function load_svmlight_file() to load a specific file\n",
    "#I Adjust maximum number of observations and variables when necessary (big file)\n",
    "#J Stop when we reached 5 files  \n",
    "#K print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 5: data modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a model to distinguish the malicious from the normal urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.98      0.98      0.98     14227\n",
      "          1       0.96      0.95      0.96      5773\n",
      "\n",
      "avg / total       0.97      0.97      0.97     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = [-1,1] #A\n",
    "sgd = SGDClassifier(loss=\"log\")  #B\n",
    "n_features=3231960 #C\n",
    "split = 20 #D\n",
    "i = 0 #E\n",
    "for tarinfo in tar: #F\n",
    "    if i  > split:\n",
    "        break\n",
    "    if tarinfo.isfile():\n",
    "        f = tar.extractfile(tarinfo.name)#G\n",
    "        X,y = load_svmlight_file(f,n_features=n_features) #G\n",
    "        if i < split:\n",
    "            sgd.partial_fit(X, y, classes=classes) #H\n",
    "        if i == split:\n",
    "            print classification_report(sgd.predict(X),y) #I\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A The target variable can be 1 or -1.  “1”: website is safe to visit, “-1”: website is unsafe \n",
    "#B Set up the stochastic gradient classifier\n",
    "#C we know the number of features from data exploration\n",
    "#D stop at the twentieth file (instead of all of them: for demonstration purposes)\n",
    "#E initialize filecounter at 0\n",
    "#F All files together take up around 2.05 Gb. The trick here is to leave the data compressed in main memory, only unpack what you need\n",
    "#G We unpack the files one by one to reduce the memory needed\n",
    "#H Use a helper function load_svmlight_file() to load a specific file\n",
    "#I third important thing is the online algorithm: it can be fed data points file by file (batches)\n",
    "#J Stop when we reached 5 files and print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Recommender system in database case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remark: \n",
    "#To install MySQLdb for windows, run the following script in command line with your envirronement activated:\n",
    "#conda install --channel https://conda.binstar.org/krisvanneste mysql-python\n",
    "\n",
    "#Check out following stackoverflow post in case this doesn't work for you or you are using Linux:\n",
    "#http://stackoverflow.com/questions/26705890/cannot-import-mysqldb-python-windows-8-1\n",
    "#conda install binstar\n",
    "#binstar search -t conda mysql-python\n",
    "\n",
    "#Also: feel free to use SQLalchemy instead of MySQLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remark 2:\n",
    "#local MySQL database needs to be setup and a schema needs to be present inside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 1 : research goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of this case study is to recommend movies to people based on their previous viewing behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 2: retrieving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will create the data ourselves for this case study so no real data retrieval. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 3: Data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import MySQLdb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####database connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = 'root' #A\n",
    "password = 'maiton' #A\n",
    "database = 'test' #A\n",
    "mc = MySQLdb.connect('localhost',user,password,database) #A\n",
    "cursor = mc.cursor() #A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A First we establish the connection; you’ll need to fill out your own username, password and schemaname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda\\envs\\book\\lib\\site-packages\\pandas\\core\\generic.py:1002: FutureWarning: The 'mysql' flavor with DBAPI connection is deprecated and will be removed in future versions. MySQL will be further supported with SQLAlchemy connectables.\n",
      "  dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "nr_customers = 100 #B\n",
    "colnames = [\"movie%d\" %i for i in range(1,33)] #B\n",
    "pd.np.random.seed(2015) #B\n",
    "generated_customers = pd.np.random.randint(0,2,32 * nr_customers).reshape(nr_customers,32) #B\n",
    "\n",
    "data = pd.DataFrame(generated_customers, columns = list(colnames)) #C\n",
    "data.to_sql('cust',mc, flavor = 'mysql', index = True, if_exists = 'replace', index_label = 'cust_id') #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie1</th>\n",
       "      <th>movie2</th>\n",
       "      <th>movie3</th>\n",
       "      <th>movie4</th>\n",
       "      <th>movie5</th>\n",
       "      <th>movie6</th>\n",
       "      <th>movie7</th>\n",
       "      <th>movie8</th>\n",
       "      <th>movie9</th>\n",
       "      <th>movie10</th>\n",
       "      <th>...</th>\n",
       "      <th>movie23</th>\n",
       "      <th>movie24</th>\n",
       "      <th>movie25</th>\n",
       "      <th>movie26</th>\n",
       "      <th>movie27</th>\n",
       "      <th>movie28</th>\n",
       "      <th>movie29</th>\n",
       "      <th>movie30</th>\n",
       "      <th>movie31</th>\n",
       "      <th>movie32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie1  movie2  movie3  movie4  movie5  movie6  movie7  movie8  movie9  \\\n",
       "0       0       0       0       0       1       0       1       0       0   \n",
       "1       0       0       0       1       0       1       1       1       0   \n",
       "\n",
       "   movie10   ...     movie23  movie24  movie25  movie26  movie27  movie28  \\\n",
       "0        0   ...           1        0        1        0        1        1   \n",
       "1        0   ...           1        1        1        0        1        1   \n",
       "\n",
       "   movie29  movie30  movie31  movie32  \n",
       "0        0        1        1        0  \n",
       "1        0        1        0        0  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#B Next we simulate a database with customers and create a few observations \n",
    "#C Store the data inside a pandas dataframe and write the dataframe in a MySQL table called \"cust\". If this table already exists, replace it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create bit strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createNum(x1,x2,x3,x4,x5,x6,x7,x8): #A\n",
    "    return  [int('%d%d%d%d%d%d%d%d' % (i1,i2,i3,i4,i5,i6,i7,i8),2) \n",
    "for (i1,i2,i3,i4,i5,i6,i7,i8) in zip(x1,x2,x3,x4,x5,x6,x7,x8)]\n",
    "\n",
    "assert int('1111',2) == 15 #B\n",
    "assert int('1100',2) == 12 #B\n",
    "assert createNum([1,1],[1,1],[1,1],[1,1],[1,1],[1,1],[1,0],[1,0]) == [255,252] #B\n",
    "\n",
    "store = pd.DataFrame() #C\n",
    "store['bit1'] = createNum(data.movie1, data.movie2,data.movie3,data.movie4,data.movie5,\n",
    "data.movie6,data.movie7,data.movie8) #C\n",
    "store['bit2'] = createNum(data.movie9, data.movie10,data.movie11,data.movie12,data.movie13,\n",
    "data.movie14,data.movie15,data.movie16) #C\n",
    "store['bit3'] = createNum(data.movie17, data.movie18,data.movie19,data.movie20,data.movie21,\n",
    "data.movie22,data.movie23,data.movie24) #C\n",
    "store['bit4'] = createNum(data.movie25, data.movie26,data.movie27,data.movie28,data.movie29,\n",
    "data.movie30,data.movie31,data.movie32) #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bit1</th>\n",
       "      <th>bit2</th>\n",
       "      <th>bit3</th>\n",
       "      <th>bit4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>42</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>223</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bit1  bit2  bit3  bit4\n",
       "0    10    62    42   182\n",
       "1    23    28   223   180"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A We represent the string as a numeric value. The string will be a concatenation of zeros (0) and ones (1) \n",
    "# as these indicate whether someone has seen a certain movie or not. The strings are then regarded as bitcode.\n",
    "# For example: 0011 is the same as the number 3\n",
    "# what def createNum() does: take in 8 values, concatenate these 8 column values and turn into a string. Then turn the bytecode of the string into a number\n",
    "\n",
    "#B Test is the function works correctly\n",
    "# binary code 1111 is the same as 15 (=1*8+1*4+1*2+1*1)\n",
    "# if the assert fails, it will raise an assert error, otherwise nothing will happen.\n",
    "\n",
    "#C Translate the movie column to 4 bit-strings in numeric form. each bitstring represents 8 movies. 4 *8 = 32 movies.\n",
    "#REMARK: you could simply use a 32bit string instead of 4*8  (its to keep the code short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create hash functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hash_fn(x1,x2,x3): #A\n",
    "    return [b'%d%d%d' % (i,j,k) for (i,j,k) in zip(x1,x2,x3)]\n",
    "\n",
    "assert hash_fn([1,0],[1,1],[0,0]) == [b'110',b'010'] #B\n",
    "\n",
    "store['bucket1'] = hash_fn(data.movie10, data.movie15,data.movie28) #C\n",
    "store['bucket2'] = hash_fn(data.movie7, data.movie18,data.movie22) #C\n",
    "store['bucket3'] = hash_fn(data.movie16, data.movie19,data.movie30) #C\n",
    "store.to_sql('movie_comparison',mc, flavor = 'mysql', index = True, \n",
    "             index_label = 'cust_id', if_exists = 'replace') #D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bit1</th>\n",
       "      <th>bit2</th>\n",
       "      <th>bit3</th>\n",
       "      <th>bit4</th>\n",
       "      <th>bucket1</th>\n",
       "      <th>bucket2</th>\n",
       "      <th>bucket3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>42</td>\n",
       "      <td>182</td>\n",
       "      <td>011</td>\n",
       "      <td>100</td>\n",
       "      <td>011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>223</td>\n",
       "      <td>180</td>\n",
       "      <td>001</td>\n",
       "      <td>111</td>\n",
       "      <td>001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bit1  bit2  bit3  bit4 bucket1 bucket2 bucket3\n",
       "0    10    62    42   182     011     100     011\n",
       "1    23    28   223   180     001     111     001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A Define the hash function (it is exactly like the createNum() function without the final conversion to a number and for 3 columns instead of 8)\n",
    "#B Test if it works correctly (if no error is raised, it works)\n",
    "#it's sampling on columns but all observations will be selected\n",
    "#C Create hash values from customer movies, respectively [10,5,28],[7,18,22],[16,19,30]\n",
    "#D store this information in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createIndex(column, cursor): #A\n",
    "    sql = 'CREATE INDEX %s ON movie_comparison (%s);' % (column, column)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "createIndex('bucket1',cursor)  #B\n",
    "createIndex('bucket2',cursor)  #B\n",
    "createIndex('bucket3',cursor)  #B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A Create a function to easily create indices. indices will quicken retrieval\n",
    "#B actually put the index on the bit buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 4: data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###This step would be pretty redundent since we created the data ourselve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 5: model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create hamming distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(1304, 'FUNCTION HAMMINGDISTANCE already exists')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-577c30d317df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m   BIT_COUNT(A3 ^ B3); '''\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSql\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m#A\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m Sql = '''Select hammingdistance(\n",
      "\u001b[1;32mC:\\Users\\Gebruiker\\Anaconda\\envs\\book\\lib\\site-packages\\MySQLdb\\cursors.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, query, args)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrorhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_executed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_defer_warnings\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warning_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gebruiker\\Anaconda\\envs\\book\\lib\\site-packages\\MySQLdb\\connections.pyc\u001b[0m in \u001b[0;36mdefaulterrorhandler\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0merrorclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrorvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mre_numeric_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"^(\\d+)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: (1304, 'FUNCTION HAMMINGDISTANCE already exists')"
     ]
    }
   ],
   "source": [
    "Sql = '''\n",
    "CREATE FUNCTION HAMMINGDISTANCE(\n",
    "  A0 BIGINT, A1 BIGINT, A2 BIGINT, A3 BIGINT, \n",
    "  B0 BIGINT, B1 BIGINT, B2 BIGINT, B3 BIGINT\n",
    ")\n",
    "\n",
    "RETURNS INT DETERMINISTIC\n",
    "RETURN \n",
    "  BIT_COUNT(A0 ^ B0) +\n",
    "  BIT_COUNT(A1 ^ B1) +\n",
    "  BIT_COUNT(A2 ^ B2) +\n",
    "  BIT_COUNT(A3 ^ B3); '''\n",
    "#A\n",
    "cursor.execute(Sql) #B\n",
    "\n",
    "Sql = '''Select hammingdistance(\n",
    "    b'11111111',b'00000000',b'11011111',b'11111111'\n",
    ",b'11111111',b'10001001',b'11011111',b'11111111'\n",
    ")''' #C\n",
    "pd.read_sql(Sql,mc) #D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A Define the function. It takes 8 input arguments: 4 strings of length 8 for the first customer and another 4 strings of length 8 for the second customer. This way we can compare 2 customers side by side for 32 movies.\n",
    "#B The function is stored in database. You can only do this once, running this code a  second time will result in an error: OperationalError: (1304, 'FUNCTION HAMMINGDISTANCE already exists') \n",
    "#C To check this function you can run this SQL statement with 8 fixed strings. notice the \"b\" before each string, indicating that you’re passing bit values. The outcome of this particular test should be 3 which indicates the series of strings differ in only 3 places.\n",
    "#D this actually runs the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 6: presentation & automatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point we have an application that use the database to lookup a customer and present him with movies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Finding similar customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cust_id  distance\n",
       "0       27         0\n",
       "1        2         8\n",
       "2       97         9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_id = 27 #A\n",
    "sql = \"select * from movie_comparison where cust_id = %s\" % customer_id  #A\n",
    "cust_data = pd.read_sql(sql,mc) #A\n",
    "sql =  \"\"\" select cust_id,hammingdistance(bit1,\n",
    "bit2,bit3,bit4,%s,%s,%s,%s) as distance\n",
    "           from movie_comparison where bucket1 = '%s' or bucket2 ='%s' \n",
    "or bucket3='%s' order by distance limit 3\"\"\" % (cust_data.bit1[0],cust_data.bit2[0], \n",
    "cust_data.bit3[0], cust_data.bit4[0],\n",
    "       cust_data.bucket1[0], cust_data.bucket2[0],cust_data.bucket3[0])#B\n",
    "shortlist = pd.read_sql(sql,mc) #C\n",
    "shortlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A pick a customer from database\n",
    "#B we do 2-step sampling :\n",
    "# first sampling: Index must be exactly the same as the one of the selected customer (is based on 9 movies). selected people must have seen (or not seen) these 9 movies\n",
    "# like our customer did. \n",
    "# second is a ranking based on the 4 bit strings. These take into account all the movies in the database. \n",
    "#C we show the 3 customers most like customer 27. Of course customer 27 ends up first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Finding an unseen movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cust_id</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie31</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1   2\n",
       "cust_id  2  27  97\n",
       "movie3   0   1   1\n",
       "movie9   0   1   1\n",
       "movie11  0   1   1\n",
       "movie12  1   0   0\n",
       "movie15  1   0   0\n",
       "movie16  0   1   1\n",
       "movie25  0   1   1\n",
       "movie31  1   0   0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust = pd.read_sql('select * from cust where cust_id in (27,2,97)',mc)#A\n",
    "dif = cust.T #B\n",
    "dif[dif[0] != dif[1]] #C\n",
    "#A Select movies that customer 27,2,97 have seen\n",
    "#B Transpose for convenience\n",
    "#C Select movies that customer 27 didn’t see yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
