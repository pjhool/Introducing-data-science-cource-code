{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: data preparation of loans data for self service BI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>REMARK:</b> For this case study we will be using the hortonbox (the version was 2.3.2 at the time of writing this chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to do on the hortonbox before the system works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>REMARK:</b> you can connect to the hortonbox via putty, this is usually easier than working on the VM itself</p>\n",
    "<p>yum -y install python-pip</p>\n",
    "<p>pip install git+https://github.com/DavyCielen/pywebhdfs.git --upgrade</p>\n",
    "<p> <b>REMARK:</b> this custom repository is a fork of pywebhdfs. pywebhdfs was broken at the time of writing. A reqest to fix it has been issued but until the main repo works properly again, use the custom one we created</p>\n",
    "<p>pip install pandas</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>REMARK:</b> all the following <b>code</b> will need to be run <b>on the hortonbox</b> itself</p>\n",
    "<p>To do this, open a putty session and issue the \"pyspark\" command, this will open a python interpreter</p>\n",
    "<p>If all went well before, this interpreter should also have the pandas and pywebhdfs (or adapted pywebhdfs) libraries </p>\n",
    "<p>All the following code should be run in this pyspark session (you can just copy paste it piece by piece)</p>\n",
    "<p>There are other options like copying the code to a .py file and running it there, another option is the zeppelin notebook</p>\n",
    "<p>Zeppelin is installed on hortonbox but was not mature enough to rely on at the time of writing.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 of data science process : research goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Main learning goal:</b> Using big data technologies to preparation data for <b>self-service BI</b>. In this case study we will not get into model building but instead spend some more time on the data preparation in order to allow other people to explore it and eventually create reports using the prepared data. The self-service BI concept allows end-users to find their own insights and is often applied when the company is very data driven but not enough data scientists are present to provide the insights. Chapter 9 will also feature a case study with some more focus on the application of the dashboard layer for self service BI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 of data process : data retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import StringIO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = requests.get(\"https://resources.lendingclub.com/LoanStats3d.csv.zip\", verify=False) #A\n",
    "stringio = StringIO.StringIO(source.content) #B\n",
    "unzipped = zipfile.ZipFile(stringio) #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A Download the data from the lending club. This https so it should verify but we won't bother (verify=False) \n",
    "#B creates a virtual file.\n",
    "#C Unzip the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move data to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pywebhdfs.webhdfs import PyWebHdfsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subselection_csv = pd.read_csv(unzipped.open('LoanStats3d.csv'),skiprows=1,skipfooter=2,engine='python') #A\n",
    "stored_csv = subselection_csv.to_csv('./stored_csv.csv') #B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs = PyWebHdfsClient(user_name=\"hdfs\",port=50070,host=\"sandbox\")#C\n",
    "hdfs.make_dir('chapter5')#D\n",
    "with open('./stored_csv.csv') as file_data: #E\n",
    "        hdfs.create_file('chapter5/LoanStats3d.csv',file_data, overwrite=True)#F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A Do some preliminary data cleaning using Pandas: remove top row and bottom 2 rows because they are useless to us. simply opening the original file will show you this.\n",
    "#B Store it locally because we need to transfer it to the hadoop file system\n",
    "#C Connect to hadoop sandbox\n",
    "#D Create a folder called \"chapter5\" on hadoop filesystem\n",
    "#E Open the locally stored csv \n",
    "#F Create the .csv file on hadoop filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print hdfs.get_file_dir_status('chapter5/LoanStats3d.csv')#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A print the file status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hdfs.delete_file_dir('chapter5/LoanStats3d.csv',recursive=True)#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A code in case you want to delete a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 of data process : data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext #A\n",
    "from pyspark.sql import HiveContext #B\n",
    "#sc = SparkContext()#C\n",
    "sqlContext = HiveContext(sc)#D\n",
    "data = sc.textFile(\"/chapter5/LoanStats3d.csv\") #E\n",
    "parts = data.map(lambda r:r.split(',')) #F\n",
    "firstline = parts.first() #G\n",
    "datalines = parts.filter(lambda x:x != firstline)#H\n",
    "def cleans(row): #I\n",
    "        row[7] = str(float(row[7][:-1])/100)#J\n",
    "        return [s.encode('utf8').replace(r\"_\",\" \").lower() for s in row]#K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datalines = datalines.map(lambda x: cleans(x))#L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A import Spark context --> not necessary when working directly in pyspark!\n",
    "#B import Hive context\n",
    "#C In the pyspark session the sparkcontext is automatically present. In  other cases (zeppelin notebook) you will need to \n",
    "# create this explicitly\n",
    "#D Create the hivecontext \n",
    "#E load in the dataset from the hadoop directory\n",
    "#F split the dataset with the komma (,) delimiter. This is the end of line delimiter for this file\n",
    "#G grab the first line\n",
    "#H grab all the lines but the first line. because the first line is just variable names\n",
    "#I the cleaning function will use the power of Spark to clean the data. The input of this function will be a line of data.\n",
    "#J column 8 (index = 7) has % formatted numbers. We don't need that '%' sign.\n",
    "#K Encode everything in utf8, replace underscores with spaces and lowercase everything.\n",
    "#L excecute the data cleaning line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * #A\n",
    "fields = [StructField(field_name,StringType(),True) for field_name in firstline] #B\n",
    "schema = StructType(fields) #C\n",
    "schemaLoans = sqlContext.createDataFrame(datalines, schema) #D\n",
    "schemaLoans.registerTempTable(\"loans\") #E\n",
    "\n",
    "sqlContext.sql(\"drop table if exists LoansByTitle\") #F\n",
    "sql = '''create table LoansByTitle stored as parquet as select title, count(1) as number from loans group by title order by number desc'''#F\n",
    "sqlContext.sql(sql)#F\n",
    "\n",
    "sqlContext.sql('drop table if exists raw') #G\n",
    "sql = '''create table raw stored as parquet as select title, emp_title,grade,home_ownership,int_rate,recoveries,collection_recovery_fee,loan_amnt,term from loans'''#G\n",
    "sqlContext.sql(sql)#G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A Import sql data types \n",
    "#B Create metadata: the Spark SQL StructField function represents a field in a StructType. \n",
    "# StructField object is comprised of three fields: name (a string), dataType (a DataType) and 'nullable' (a boolean). \n",
    "#The field of name is the name of a StructField. \n",
    "#The field of dataType specifies the data type of a StructField.\n",
    "#The field of nullable specifies if values of a StructField can contain None values.\n",
    "#C StructType function creates the data schema. A StructType object requires a list of StructFields as imput.\n",
    "#D Create a dataframe from the data (datalines) and the dataschema (schema)\n",
    "#E Register it as a table called loans \n",
    "#F Drop table (in case it already exists), summarize and store in hive. LoansByTitle represents the sum of loans by job title\n",
    "#G Drop table (in case it already exists) and store a subset of the raw data in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 of data process : data exploration & part 6: presentation to end-user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data exploration part of this case study will be done in Qlick. There is no real modelling phase in this case study. Because Qlick allow you to build dashboard and reports, part 4 and 6 form an overlap. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
